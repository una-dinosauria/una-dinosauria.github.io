<!doctype html>
<html lang="en">
	<!--
                       __    _       _ _      _        _                   _         _ _
                      / _)  (_)_   _| (_) ___| |_ __ _( )__  __      _____| |__  ___(_) |_ ___
             _.----._/ /    | | | | | | |/ _ \ __/ _` |/ __| \ \ /\ / / _ \ '_ \/ __| | __/ _ \
            /         /     | | |_| | | |  __/ || (_| |\__ \  \ V  V /  __/ |_) \__ \ | ||  __/
         __/ (  | (  |     _/ |\__,_|_|_|\___|\__\__,_||___/   \_/\_/ \___|_.__/|___/_|\__\___|
        /__.-'|_|--|_|    |__/
	-->
  	<head>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-17RE2EP8JX"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-17RE2EP8JX');
		</script>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="description" content="Julieta Martinez - Research Scientist at Meta Reality Labs Research, working on computer vision and machine learning. Publications on 3D human pose estimation, motion prediction, neural network compression, and self-driving car localization.">

		<!-- Open Graph / Social Media -->
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://una-dinosauria.github.io/">
		<meta property="og:title" content="Julieta Martinez - Research Scientist">
		<meta property="og:description" content="Research Scientist at Meta Reality Labs Research, working on computer vision and machine learning.">

		<!-- Twitter Card -->
		<meta name="twitter:card" content="summary">
		<meta name="twitter:title" content="Julieta Martinez - Research Scientist">
		<meta name="twitter:description" content="Research Scientist at Meta Reality Labs Research, working on computer vision and machine learning.">

		<!-- bootswatch css -->
		<link rel="stylesheet" href="css/bootstrap.min.css">
		<!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BmbxuPwQa2lc/FVzBcNJ7UAyJxM6wuqIj61tLrc4wSX0szH/Ev+nYRRuWlolflfl" crossorigin="anonymous"> -->
		<!-- font awesome icons -->
		<script src="https://use.fontawesome.com/89e6d71bc3.js"></script>
		<!-- academicons -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

		<title>Julieta :D</title>
  	</head>

  	<body>
		<div class="bg-light p-5 mb-4">
			<div class="container">
				<h1 class="display-3">Hi! I'm Julieta :D</h1>
				<p class="lead">In 2022 I joined <a href="https://tech.fb.com/ar-vr/">Reality Labs Research</a> at <a href="http://meta.com/">Meta</a>.</p>
				<p class="lead">From 2018 to 2020 I was at <a href="https://www.uber.com/info/atg/">Uber ATG</a> Toronto, working with <a href="https://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a>.
				Then in 2021 I followed the team to <a href="https://waabi.ai/">Waabi</a>.
				</p>
				<p class="lead">In the fall of 2016 I visited <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael Black</a> and <a href="https://ps.is.tuebingen.mpg.de/person/jromero/">Javier Romero</a> in the <a href="https://ps.is.tuebingen.mpg.de/">Perceiving Systems</a> group at <a href="https://is.tuebingen.mpg.de/">MPI Tuebingen</a>.</p>
				<p class="lead">From 2015 to 2018 I was a PhD student in the <a href="https://www.cs.ubc.ca">Department of Computer Science</a>
				at the <a href="https://www.ubc.ca">University of British Columbia</a>,
				supervised by <a href="https://www.cs.ubc.ca/~little">Jim Little</a> and <a href="https://www.cs.ubc.ca/~hoos">Holger Hoos</a>.
				</p>
				<p class="lead">From 2012 to 2014 I got an MSc from the same university, working with the same nice people.</p>
				<p class="lead">Before that I was in <a href="https://www.youtube.com/watch?v=JsUt2jsLM1k">Mexico</a>.</p>
				<p class="lead">
				<a href="mailto:jltmtzc@gmail.com">email</a> /
				<a href="https://bsky.app/profile/yoknapathawa.bsky.social">bluesky</a> /
				<a href="https://github.com/una-dinosauria">github</a> /
				<a href="https://scholar.google.ca/citations?user=XFFTY50AAAAJ&hl=en">google scholar</a> /
				<a href="https://www.linkedin.com/in/julmtz/">linkedin</a>
				</p>
			</div>
		</div>

		<main>
		<div class="container">
			<h2>Selected papers</h2>
			<div class="row" data-masonry='{"percentPosition": true }'>

				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/eccv24.gif"><img class="card-img-top img-fluid" src="imgs/eccv24.gif" alt="Sapiens model architecture showing foundation model for human vision tasks"></a>
						<div class="card-body">
							<h4 class="card-title">Foundation for human vision models</h4>
							<p class="card-text">
							Rawal Khirodkar, Timur Bagautdinov, <strong>Julieta Martinez</strong>, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito<br>
							<em>Sapiens: Foundation for Human Vision Models.</em>
							In <strong>ECCV 2024 (best paper award candidate)</strong><br />
							A very large MAE that, when fine-tuned, gets state of the art performance on a wide range of human vision tasks.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/abs/2408.12569.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/2408.12569" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://github.com/facebookresearch/sapiens" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="papers/bibtex/eccv24.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/cvpr21.png"><img class="card-img-top img-fluid" src="imgs/cvpr21.png" alt="Neural network compression via permutation and quantization"></a>
						<div class="card-body">
							<h4 class="card-title">Neural network compression</h4>
							<p class="card-text">
							<strong>Julieta Martinez*</strong>, Jashan Shewakramani*, Ting Wei Liu*, Ioan Andrei Bârsan, Wenyuan Zeng, and Raquel Urtasun.<br>
							<em>Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks.</em>
							In <strong>CVPR 2021 (oral presentation)</strong><br />
							We search for functionally-equivalent, yet easier to compress networks to achieve state-of-the-art memory-to-accuracy tradeoffs in image classification and object detection.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/2010.15703.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/2010.15703" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://github.com/uber-research/permute-quantize-finetune" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="papers/bibtex/cvpr21.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>

				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/arxiv21.png"><img class="card-img-top img-fluid" src="imgs/arxiv21.png" alt="Architecture diagram for joint localization, perception, and prediction"></a>
						<div class="card-body">
							<h4 class="card-title">Joint localization, perception, and prediction</h4>
							<p class="card-text">
							John Phillips, <strong>Julieta Martinez</strong>, Ioan Andrei Bârsan, Sergio Casas, Abbas Sadat, and Raquel Urtasun.<br>
							<em>Deep Multi-Task Learning for Joint Localization, Perception, and Prediction.</em>
							In <strong>CVPR 2021</strong><br />
							We design an architecture that jointly performs vehicle ego-localization, object detection, and motion forecasting.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/2101.06720.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/2101.06720" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="papers/bibtex/arxiv21.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>

				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/icra20.png"><img class="card-img-top img-fluid" src="imgs/icra20.jpg" alt="Pit30M dataset showing Pittsburgh street scenes for localization"></a>
						<div class="card-body">
							<h4 class="card-title">Pit30M</h4>
							<p class="card-text">
							<strong>Julieta Martinez</strong>, Sasha Doubov, Jack Fan, Ioan Andrei Bârsan, Shenlong Wang, Gellért Máttyus, and Raquel Urtasun.<br>
							<em>Pit30M: A Benchmark for Global Localization in the Age of Self-Driving Cars.</em>
							In <strong>ICRA 2020 (best application paper runner-up)</strong><br>
							We propose a dataset of 30 million images and LiDAR pairs to benchmark localization at city scale.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/2012.12437.pdf"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/2012.12437"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://github.com/pit30m/pit30m"><i class="fa fa-github fa-lg"></i></a>
							<a href="https://youtu.be/hJ6A_1YSITo"><i class="fa fa-youtube-play fa-lg"></i></a>
							<a href="papers/bibtex/icra20.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/cvpr19.png"><img class="card-img-top img-fluid" src="imgs/cvpr19.png" alt="Compressed binary maps for low-memory localization"></a>
						<div class="card-body">
							<h4 class="card-title">Low-memory localization</h4>
							<p class="card-text">
							Xinkai Wei*, Ioan Andrei Bârsan*, Shenlong Wang*, <strong>Julieta Martinez</strong>, and Raquel Urtasun.<br>
							<em>Learning to Localize through Compressed Binary Maps.</em>
							In <strong>CVPR 2019</strong><br>
							We train neural networks to learn very low-memory maps useful for localization in self-driving.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/2012.10942.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/2012.10942" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://www.youtube.com/watch?v=vL9F6qfwBFk" aria-label="YouTube video"><i class="fa fa-youtube-play fa-lg"></i></a>
							<a href="papers/bibtex/cvpr19.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<div class="card-body">
							<h4 class="card-title">Faster and more accurate compressed nearest neighbour search</h4>
							<p class="card-text">
							<strong>Julieta Martinez</strong>, Shobhit Zakhmi, Holger H. Hoos and James J. Little.<br>
							<em>LSQ++: lower running time and higher recall in multi-codebook quantization.</em>
							In <strong>ECCV 2018</strong><br>
							We benchmark multi-codebook quantization (MCQ) approaches on an equal footing and propose two improvements that make MCQ faster and more accurate.</p>
						</div>
						<div class="card-footer">
							<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Julieta_Martinez_LSQ_lower_runtime_ECCV_2018_paper.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://github.com/una-dinosauria/Rayuela.jl" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="papers/bibtex/eccv18.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/iccv17.png"><img class="card-img-top img-fluid" src="imgs/iccv17.png" alt="3D human pose estimation from 2D joint detections"></a>
						<div class="card-body">
							<h4 class="card-title">3d pose baseline</h4>
							<p class="card-text">
							<strong>Julieta Martinez</strong>, Rayat Hossain, Javier Romero and James J. Little.<br>
							<em>A simple yet effective baseline for 3d human pose estimation.</em>
							In <strong>ICCV 2017</strong> (28.9% acceptance rate)<br>
							We propose a simple deep learning baseline for 3d human pose estimation that outperforms the state of the art.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/1705.03098.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/1705.03098" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://github.com/una-dinosauria/3d-pose-baseline" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="https://www.youtube.com/watch?v=Hmi3Pd9x1BE" aria-label="YouTube video"><i class="fa fa-youtube-play fa-lg"></i></a>
							<a href="papers/bibtex/arxiv17.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/cvpr17.png"><img class="card-img-top img-fluid" src="imgs/cvpr17.png" alt="Recurrent neural network for human motion prediction"></a>
						<div class="card-body">
							<h4 class="card-title">Human motion prediction</h4>
							<p class="card-text">
							<strong>Julieta Martinez</strong>, Michael J. Black, Javier Romero.<br>
							<em>On human motion prediction using recurrent neural networks.</em>
							In <strong>CVPR 2017</strong> (29.84% acceptance rate)<br>
							We take a close look at deep recurrent approaches for human motion prediction, and propose a simple and scalable architecture that outperforms the state of the art.</p>
						</div>
						<div class="card-footer">
							<a href="https://arxiv.org/pdf/1705.02445.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://arxiv.org/abs/1705.02445" aria-label="arXiv"><i class="ai ai-arxiv ai-lg"></i></a>
							<a href="https://github.com/una-dinosauria/human-motion-prediction" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="https://youtu.be/JMFNws70onI" aria-label="YouTube video"><i class="fa fa-youtube-play fa-lg"></i></a>
							<a href="papers/bibtex/cvpr17.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/eccv16.png"><img class="card-img-top img-fluid" src="imgs/eccv16.png" alt="Additive quantization for compressed nearest neighbour search"></a>
						<div class="card-body">
							<h4 class="card-title">Compressed nearest neighbour search</h4>
							<p class="card-text"><strong>Julieta Martinez</strong>, Joris Clement, Holger H. Hoos, James J. Little.<br>
							<em>Revisiting additive quantization.</em>
							In <strong>ECCV 2016</strong> (26.6% acceptance rate) <br>
							Additive quantization (AQ) is a promising vector compression approach for large-scale approximate nearest neighbour search. We introduce an optimization method for AQ that pushes it beyond the state of the art.</p>
						</div>
						<div class="card-footer">
							<a href="papers/eccv16.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://github.com/una-dinosauria/local-search-quantization" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="papers/bibtex/eccv16.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
				<div class="col-sm-6 col-lg-4 mb-4">
					<div class="card">
						<a href="imgs/cvpr14.png"><img class="card-img-top img-fluid" src="imgs/cvpr14.png" alt="3D pose estimation from motion for cross-view action recognition"></a>
						<div class="card-body">
							<h4 class="card-title">3d pose from motion</h4>
							<p class="card-text">Ankur Gupta*, <strong>Julieta Martinez</strong>*, James J. Little, Robert J. Woodham.<br>
							<em>3D pose from motion for cross-view action recognition.</em>
							In <strong>CVPR 2014</strong> (29.88% acceptance rate)<br>
							An approach to improving cross-view action recognition by retrieving mocap given video sequences.</p>
						</div>
						<div class="card-footer">
							<a href="papers/cvpr14.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://github.com/ubc-cvlab/mocap-dense-trajectories" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
							<a href="https://www.youtube.com/watch?v=7hur0Vr-7aA" aria-label="YouTube video"><i class="fa fa-youtube-play fa-lg"></i></a>
							<a href="videos/mocap-trajectories-video.mp4" aria-label="Video file"><i class="fa fa-file-video-o fa-lg" data-wow-delay=".5s"></i></a>
							<a href="papers/bibtex/ankur_cvpr_14.html" target="_blank">.bib</a>
						</div>
					</div>
				</div>
			</div>

			<!-- Projects Row -->
			<h2>Misc</h2>
			<div class="row">
				<div class="col-md-4">
					<div class="card">
						<a href="https://github.com/una-dinosauria/deepviz"><img class="card-img-top img-fluid" src="imgs/dviz.png" alt="DeepViz visualization tool interface for image retrieval"></a>
						<div class="card-body">
							<h4 class="card-title">DeepViz</h4>
							<p class="card-text">
							My information visualization final project, taught by the wonderful <a href="https://www.cs.ubc.ca/~tmm/">Tamara Munzner</a>. A javascript visualization tool for image retrieval (2015).</p>
						</div>
						<div class="card-footer">
							<a href="https://www.cs.ubc.ca/~tmm/courses/547-15/projects/julieta/report.pdf" aria-label="PDF"><i class="fa fa-file-pdf-o fa-lg"></i></a>
							<a href="https://github.com/una-dinosauria/deepviz" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
						</div>
					</div>
					<br/>
				</div>
				<div class="col-md-4">
					<div class="card">
						<a href="https://una-dinosauria.github.io/efros-and-leung-js/"><img class="card-img-top img-fluid" src="imgs/donkey.jpg" alt="Texture synthesis example using Efros and Leung algorithm"></a>
						<div class="card-body">
							<h4 class="card-title">Efros and Leung JS</h4>
							<p class="card-text">
							A javascript implementation of a <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/papers/efros-iccv99.pdf">classic method for texture synthesis</a> (2015).</p>
						</div>
						<div class="card-footer">
							<a href="https://github.com/una-dinosauria/efros-and-leung-js/" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
						</div>
					</div>
					<br/>
				</div>
				<div class="col-md-4">
					<div class="card">
						<a href="http://una-dinosauria.github.io/game%20theory/2014/09/03/better-than-tit-for-that"><img class="card-img-top img-fluid" src="imgs/pavlov.png" alt="Evolutionary dynamics simulation of Pavlov strategy"></a>
						<div class="card-body">
							<h4 class="card-title">Pavlov is no simpleton</h4>
							<p class="card-text">
							I tried to reproduce the results of a <a href="http://abel.math.harvard.edu/archive/153_fall_04/Additional_reading_material/A_strategy_of_winstay_loseshift_that_out_performs_titfortat_in_the_Prisoners_Dilemma_game.pdf.pdf">1993 paper on evolutionary dynamics</a> by Sigmund and Novak. I also wrote a <a href="https://una-dinosauria.github.io/game%20theory/2014/09/03/better-than-tit-for-that">blog post</a> about it (2014).<br>
						</div>
						<div class="card-footer">
							<a href="https://github.com/una-dinosauria/better-than-tit-for-tat" aria-label="GitHub"><i class="fa fa-github fa-lg"></i></a>
						</div>
					</div>
					<br/>
				</div>
			</div>
			<br>

			<!-- NOT RESEARCH -->
			<h2>rand()</h2>
			<div class="row">
				<div class="col-lg-12">
					<div class="list-group">
						<div class="list-group-item">
							<p>I am/have served as a reviewer for CVPR, ECCV, ICCV, IROS, ICRA, NeurIPS, AAAI, IJCAI, CVIU, and TPAMI.<p>
						</div>
						<div class="list-group-item">
							<p>I have done science outreach with <a href="https://www.cs.ubc.ca/girlsmarts4tech/">GIRLsmarts4tech</a> and UBC women in science.<p>
						</div>
						<div class="list-group-item">
							<p>On the fall of 2014 I started organizing CVRG, the <a href="https://www.cs.ubc.ca/labs/lci/cvrg/">Computer Vision Reading Group</a> at UBC.</p>
						</div>
					</div>
				</div>
			</div>


			<!-- close container -->
		</div>
		</main>

		<!-- masonry -->
		<script src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous" async></script>
		<!-- bootstrap -->
		<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/js/bootstrap.bundle.min.js" integrity="sha384-b5kHyXgcpbZJO/tY9Ul7kGkf1S0CWuKcCD38l8YkeH8z8QjE0GmW1gYU5S9FOnJ0" crossorigin="anonymous"></script>
	</body>

</html>
